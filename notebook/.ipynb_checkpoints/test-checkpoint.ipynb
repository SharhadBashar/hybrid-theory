{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sharhad.bashar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from cleantext import clean\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from dython import nominal\n",
    "from scipy.stats import f_oneway\n",
    "import scipy\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, FeatureHasher, TfidfTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1v1.lol</td>\n",
       "      <td>IAB9</td>\n",
       "      <td>https://test.1v1.lol/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1v1.lol</td>\n",
       "      <td>IAB9</td>\n",
       "      <td>https://1v1.lol/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1v1.lol</td>\n",
       "      <td>IAB9</td>\n",
       "      <td>https://1v1.lol/?bxns=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1v1.lol</td>\n",
       "      <td>IAB9</td>\n",
       "      <td>https://1v1.lol./</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1v1.lol</td>\n",
       "      <td>IAB9</td>\n",
       "      <td>https://rc.1v1.lol/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20621</th>\n",
       "      <td>zuberrealty.com</td>\n",
       "      <td>IAB21</td>\n",
       "      <td>https://myhomes.zuberrealty.com/idx/details/li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20622</th>\n",
       "      <td>zuberrealty.com</td>\n",
       "      <td>IAB21</td>\n",
       "      <td>https://myhomes.zuberrealty.com/idx/details/li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20623</th>\n",
       "      <td>zuberrealty.com</td>\n",
       "      <td>IAB21</td>\n",
       "      <td>https://myhomes.zuberrealty.com/idx/details/li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20624</th>\n",
       "      <td>zuberrealty.com</td>\n",
       "      <td>IAB21</td>\n",
       "      <td>https://myhomes.zuberrealty.com/idx/details/li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20625</th>\n",
       "      <td>zuberrealty.com</td>\n",
       "      <td>IAB21</td>\n",
       "      <td>https://myhomes.zuberrealty.com/idx/details/li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20626 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                domain  label  \\\n",
       "0              1v1.lol   IAB9   \n",
       "1              1v1.lol   IAB9   \n",
       "2              1v1.lol   IAB9   \n",
       "3              1v1.lol   IAB9   \n",
       "4              1v1.lol   IAB9   \n",
       "...                ...    ...   \n",
       "20621  zuberrealty.com  IAB21   \n",
       "20622  zuberrealty.com  IAB21   \n",
       "20623  zuberrealty.com  IAB21   \n",
       "20624  zuberrealty.com  IAB21   \n",
       "20625  zuberrealty.com  IAB21   \n",
       "\n",
       "                                                    urls  \n",
       "0                                  https://test.1v1.lol/  \n",
       "1                                       https://1v1.lol/  \n",
       "2                                https://1v1.lol/?bxns=1  \n",
       "3                                      https://1v1.lol./  \n",
       "4                                    https://rc.1v1.lol/  \n",
       "...                                                  ...  \n",
       "20621  https://myhomes.zuberrealty.com/idx/details/li...  \n",
       "20622  https://myhomes.zuberrealty.com/idx/details/li...  \n",
       "20623  https://myhomes.zuberrealty.com/idx/details/li...  \n",
       "20624  https://myhomes.zuberrealty.com/idx/details/li...  \n",
       "20625  https://myhomes.zuberrealty.com/idx/details/li...  \n",
       "\n",
       "[20626 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/raw/part-00000-tid-5266091121617272357-73173b1e-ce15-42f6-a92f-e17a6426d0d7-1572-1-c000.snappy.parquet')\n",
    "df = df.explode('urls', ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['1v1.lol', '2zoo.com', '365onlinenews.com', '3dcompare.com',\n",
       "       '4chan.org', '4livedemo.com', '53.com', '7starhd.red',\n",
       "       'a-1poolservice.com', 'a2bsecurity.nl', 'abc57.com',\n",
       "       'actualcracker.com', 'adjarabet.com', 'adruby.com', 'afcic.org',\n",
       "       'affinityliving.com', 'africanplan.org',\n",
       "       'agilehealthinsurance.com', 'albany.edu',\n",
       "       'albergodiffusopaluzza.it', 'alenquerensis.blogspot.com',\n",
       "       'aliboxtv.com', 'alim.org', 'allensjewelers.com', 'allnewsf.com',\n",
       "       'alteapsicologos.com', 'alvolante.it', 'amiaux.fr', 'amplifon.com',\n",
       "       'anm.gov.my', 'annapolisvalleyproperty.com', 'apartments.com',\n",
       "       'apexlearning.com', 'apexpainspecialists.com', 'apkdl.in',\n",
       "       'appsenjoy.com', 'aralleida.com', 'ardengrange.com',\n",
       "       'arkadium.com', 'armariodoprofessor.blogspot.com', 'article.com',\n",
       "       'artscroll.com', 'artstudio.co.il', 'asortie.com',\n",
       "       'astrologie-autrement.com', 'astrostyle.com', 'asxshareprice.com',\n",
       "       'atcincometax.com', 'atheistrev.com', 'austrade.gov.au',\n",
       "       'australiandiamondbrokers.com', 'autismnow.org', 'autozel.com',\n",
       "       'azpoolsupplywarehouse.com', 'backenmachtgluecklich.de',\n",
       "       'badfolder.com', 'bangsamoro.gov.ph', 'barstoolsports.com',\n",
       "       'basementrejects.com', 'batangkab.go.id', 'bcahkitchener.com',\n",
       "       'bcbsok.com', 'beachesandbush.com.au', 'bed-wet.com',\n",
       "       'beerandbrewing.com', 'beerganzberg.com',\n",
       "       'beginningwithbergamot.com', 'bella-tucker.com', 'bensolomon.co',\n",
       "       'bestforexrobots.net', 'biasiswa.online', 'bigrock.com',\n",
       "       'bioresonantie-therapie.nl', 'bizmaker.eu', 'bizpromptinfo.com',\n",
       "       'blanquivioletas.com', 'blastkhabar.com', 'blogdomagno.com.br',\n",
       "       'blubayapartments.com', 'bm5150.com', 'bodhgemandcrystals.com',\n",
       "       'bodybuilding.com', 'bodytonix.com.au', 'bosschert.nl',\n",
       "       'boutiquehotels.co.uk', 'bovmi.com', 'box.live', 'brainfuse.com',\n",
       "       'brigittegabriel.com', 'brioweb.eu', 'broadmoorgolfclub.com',\n",
       "       'btn.co.id', 'bulawayo24.com', 'buscainmobiliarias.com',\n",
       "       'businessinsider.in', 'businesswebadmin.com',\n",
       "       'cakesbakesandcookies.com', 'campbellrentals.com',\n",
       "       'camping-domainedelanature.fr', 'camping-loperhet.com',\n",
       "       'campingartaudois.com', 'campingrail.com', 'cardana.ru',\n",
       "       'cardwells.co.uk', 'carolinaautospa.com',\n",
       "       'cartersecurityltd.co.uk', 'cary-smith.com', 'catological.com',\n",
       "       'cec18.us', 'cellinnov.com', 'centralparkbikerental.nyc',\n",
       "       'ceporn.net', 'cgmasters.net', 'channel3000.com',\n",
       "       'chasvoice.blogspot.com', 'chatbela.com', 'cheapcrafting.com',\n",
       "       'chiesacattolica.it', 'churchinthecanyon.org',\n",
       "       'cityofpataskalaohio.gov', 'classycontent.com', 'clipthaixxx.com',\n",
       "       'cloudninedogtraining.com', 'codemiles.com', 'coffeesupremacy.com',\n",
       "       'coga.be', 'coinup.org', 'collegeofdietitians.org',\n",
       "       'colorized.com', 'colorlib.com', 'colourpop.com', 'computrack.ca',\n",
       "       'cope-salesandmarketing.com', 'cottagenotes.com',\n",
       "       'crackfullgames.com', 'craftymorning.com',\n",
       "       'crazydaysandnights.net', 'creditsmart.org.au', 'criticaleye.com',\n",
       "       'crpdsports.org', 'cruiseshiprumrunners.com', 'cryptopanic.com',\n",
       "       'cscgroup.com', 'cute-girl-nude.xyz', 'cyclingnews.com',\n",
       "       'dailyreports24h.com', 'daronne.com', 'dedrankenier.nl',\n",
       "       'defensoria.gob.pe', 'degiro.nl', 'dehavilland.co.uk',\n",
       "       'dentagama.com', 'designbakerun.blogspot.com', 'destinoikigai.com',\n",
       "       'dewa.gg', 'dewinterlogistics.nl', 'diendannguoiviet.com',\n",
       "       'digino.org', 'digonzinigroup.com', 'digsshowroom.com',\n",
       "       'discoversoccer.info', 'distantlink.com', 'dltk-kids.com',\n",
       "       'doarpt.com', 'dogdayzofcalifornia.com', 'dogreference.com',\n",
       "       'dogtrainingsingapore.org', 'dolcecucinare.it',\n",
       "       'donatelife.gov.au', 'donk69.com', 'donotdisturbgardening.com',\n",
       "       'dontorrents.app', 'doortal.fr', 'downbeat.com', 'dpaw.wa.gov.au',\n",
       "       'dphoneworld.net', 'drhirani.com', 'drpciv.biz', 'drugpolicy.org',\n",
       "       'drustorynews.com', 'dsmtalk.com', 'duckhuntingchat.com',\n",
       "       'dynamicontent.net', 'earnathon.com', 'easternsuburbsderm.com.au',\n",
       "       'easydrawingseasy.com', 'ebaumsworld.com', 'echogravity.com',\n",
       "       'economictimes.com', 'ed2go.com', 'efit30.com', 'efreeware.net',\n",
       "       'elavegan.com', 'elsalvador.com',\n",
       "       'elsonidodelahierbaelcrecer.blogspot.com', 'empleoenlondres.net',\n",
       "       'empowher.com', 'enasco.com', 'epicdope.com', 'errenteria.eus',\n",
       "       'espiritbook.com.br', 'espncricinfo.com', 'esportverd.com',\n",
       "       'everythingaboutwifi.com', 'ewtnnews.com', 'excelptny.com',\n",
       "       'expressnews.com', 'expressnews.ge', 'fatrabbitcreative.com',\n",
       "       'fdkybbq.com', 'federtennis.it', 'feyenoordpings.nl',\n",
       "       'fitnessrxformen.com', 'fixthisbuildthat.com', 'flacarshows.com',\n",
       "       'fleetfeet.com', 'fleshandboners.com', 'float-digital.com',\n",
       "       'floorplans.com', 'for-minecraft.com', 'forfarscholarship.org',\n",
       "       'fox.com', 'fox4news.com', 'francepropertyshop.com',\n",
       "       'freegames66.com', 'freizeit-haus-und-garten.de', 'fugafoto.com',\n",
       "       'fullstack.com.au', 'g6hentai.com', 'game-over.com',\n",
       "       'gamescanvas.blogspot.com', 'garythomas.com', 'gaspesia.org',\n",
       "       'gaylesbiandirectory.com', 'getleado.com', 'gettyimages.com',\n",
       "       'geves.fr', 'ghoofie.com', 'girlescape.com', 'girlversusdough.com',\n",
       "       'girlxinhjav.com', 'gismeteo.lt', 'glacierhills.com',\n",
       "       'gleitschirmblog.ch', 'gm.com', 'gmnewsbd.com', 'gog.com',\n",
       "       'golyfutbol.com', 'goodslibrary.com', 'gounboxing.com',\n",
       "       'gradguard.com', 'greensboro-dentist.com', 'grizzlies.de',\n",
       "       'grylogiczne.com.pl', 'gstatic.com', 'gulagay.com',\n",
       "       'habitatbay.org', 'heatherdeep.com', 'hentaiyabu.com',\n",
       "       'hethertons.co.uk', 'hi-lohome.com', 'hiddenhearing.ie',\n",
       "       'hidrogea.es', 'highmountainhomes.org', 'homebarbasics.com',\n",
       "       'homedecorativefurniture.com', 'homefurnitureandpatio.com',\n",
       "       'homeinstead.com', 'homenideas.com', 'honeytreepreschool.com',\n",
       "       'hookahjunkie.com', 'hopeafterabortion.com',\n",
       "       'hotelesparaninos.info', 'hotelmennini.com', 'hotelvivit.com',\n",
       "       'hoydencreative.com', 'html.com', 'huntforadvice.com',\n",
       "       'hushmail.com', 'iblanguages.com', 'icon606.com', 'igpmanager.com',\n",
       "       'ikrush.com', 'imb.com.au', 'immobiliere-salat.com',\n",
       "       'imperiasport.net', 'imusic-school.com', 'indianoil.in',\n",
       "       'infield-fly.com', 'infoquest.io', 'innatgreatneck.com',\n",
       "       'inquirer.com', 'instincts.fr', 'instituteliewiesel.com',\n",
       "       'interieur.gouv.fr', 'internationalfreightservices.com.au',\n",
       "       'internationalsoccerleagueoc.com', 'invisionstudio.com',\n",
       "       'iphone-ipad-recovery.com', 'jacobskaffee.de', 'jansatta.com',\n",
       "       'jessicaadams.com', 'jetskitips.com', 'jeudegolf.org',\n",
       "       'jeunes.gouv.fr', 'jjsolution.org', 'joberealestate.com',\n",
       "       'jobvacancy.me', 'joeycrowd.com', 'jpn-study.com',\n",
       "       'judgeemily.com', 'juegos.com', 'juffrouwtaart.nl',\n",
       "       'jumpstarttrading.com', 'justfreebies.com', 'justice.govt.nz',\n",
       "       'justinrudd.com', 'kake.com', 'katelovesmakeup.com',\n",
       "       'katiehomes.co.uk', 'keshhair.com', 'kimsoku.com',\n",
       "       'kinghomes.co.uk', 'kittywire.com', 'kobokofitness.com',\n",
       "       'koreausedcars.com', 'kumioils.com', 'lassa-architects.com',\n",
       "       'latineuro.com', 'laughitout.com', 'lavieenrose.gr', 'lcrf.org',\n",
       "       'learnervegan.com', 'legendsoflearning.com', 'lemondejuif.info',\n",
       "       'lifescarousel.com', 'lilyhair.com', 'linguaholic.com',\n",
       "       'littlebigplay.com', 'littlemypee.ga', 'livingstonmtrentals.com',\n",
       "       'livingwelldallas.com', 'lobourse.com', 'localsyr.com',\n",
       "       'locklaw.com', 'logos.com.hk', 'longestshotclub.com',\n",
       "       'loopwijzer.nl', 'losarcanos.com', 'lostsaloon.com',\n",
       "       'lucyhallmassage.com', 'lugojonline.ro', 'lunaguitars.com',\n",
       "       'maariv-mobile-sync.appspot.com', 'madassgamers.com',\n",
       "       'manga-raw.club', 'marketingnews.es', 'marthastewartweddings.com',\n",
       "       'massageclinic.us', 'mathplayground.com',\n",
       "       'matthew-rowley.blogspot.com', 'md80.it', 'megatrust.co.id',\n",
       "       'meitusdk.com', 'melbournecitycleaners.com.au',\n",
       "       'meteopadovasud.net', 'metrokota.go.id', 'mhrsd.gov.sa',\n",
       "       'midinmod.com', 'mijnwoordenboek.nl', 'mik.hu',\n",
       "       'millyrecetas.blogspot.com', 'minaz.com.my',\n",
       "       'mindfulness-sanar.com', 'mirigor.com', 'mke.com', 'mnaxe.com',\n",
       "       'mobdro.cam', 'monaiic.ca', 'motleys.com', 'moviespapa.black',\n",
       "       'movieszaa.com', 'mrbnursesmemes.in', 'muzicanet.net',\n",
       "       'my-proxy.com', 'myforex-trading-inwestycje.pl',\n",
       "       'myjoyfilledlife.com', 'mylerestates.com', 'mylifebygogogoff.com',\n",
       "       'mylovelycrazylife.com', 'mythicowl.com',\n",
       "       'nationalarchives.gov.uk', 'nationalfamily.com',\n",
       "       'nationalparkcanoerental.com', 'naukrigulf.com', 'nelsongarden.dk',\n",
       "       'netweather.tv', 'neuvoo.ae', 'neuvoo.co.uk',\n",
       "       'newlifevitaminshop.com', 'newpati.com', 'newsarmada.com',\n",
       "       'newsblaze.com', 'newslite15.com', 'newstrackerdaily.com',\n",
       "       'newyorkestatejewelry.com', 'nlccoc.org', 'nma.gov.au',\n",
       "       'nonnabox.com', 'notadev.site', 'novascotiawebcams.com',\n",
       "       'novellaraimmobiliare.it', 'nugget.ca', 'nursebarb.com',\n",
       "       'nutritionjobs.com', 'nytimes.com', 'oceanhome.co.uk',\n",
       "       'offbeetproductions.com', 'okpeke.com', 'ontodaynews.com',\n",
       "       'openskycc.com', 'optimizemybrand.com', 'orangeboxasia.com',\n",
       "       'orthodoxcheyenne.org', 'otomoto.pl', 'ourladyoflight.com',\n",
       "       'parentingteensandtweens.com', 'passion-pilze-sammeln.com',\n",
       "       'passionforsavings.com', 'patentology.com.au', 'pattersonlaw.ca',\n",
       "       'pebblecreekclub.com', 'pennemes.nl', 'pertamina.com',\n",
       "       'petgearguide.com', 'petsofun.com', 'phatgiaoaluoi.com',\n",
       "       'phillison.com', 'phillyvoice.com', 'photodoto.com',\n",
       "       'photojeepers.com', 'phototraces.com', 'phreesite.com',\n",
       "       'picodotdev.github.io', 'piramidethee.nl', 'pirateship.com',\n",
       "       'pkshop.pk', 'plagasbilbao.com', 'playerhost.net', 'playmedic.com',\n",
       "       'pleasanthillbc.org', 'pocketofpreschool.blogspot.com',\n",
       "       'poesiademujeres.com', 'ponokanews.com', 'pop-japan.com',\n",
       "       'porndron.org', 'porschetaycaninfo.com', 'premierdentalco.com',\n",
       "       'prestonwoodpregnancy.org', 'princesasdebranco.blogspot.com',\n",
       "       'profesor10demates.com', 'project123movies.com',\n",
       "       'psycho-ressources.com', 'publicholidays.africa',\n",
       "       'pulspodkarpacia.pl', 'punnuptials.com', 'radiocarbon.com',\n",
       "       'rasmussen.edu', 'rbkc.gov.uk', 'rbsporting.co.uk', 'rcu.org',\n",
       "       'realaddis.com', 'realestatelakemartin.com', 'realitica.com',\n",
       "       'redserverhost.com', 'redwings-rennes.fr', 'rendfy.com',\n",
       "       'reolink.com', 'revistautopia.org', 'rmmotors.co.nz',\n",
       "       'rockersdub.in', 'rockfordrealestate.com', 'rugby-japan.jp',\n",
       "       'runfitners.com', 'rutherfordhill.com', 'ryanair.com',\n",
       "       'sacoronavirus.co.za', 'salonat10newbury.com', 'salonprices.com',\n",
       "       'samorodok.tv', 'samsung.com', 'sanluigi.roma.it',\n",
       "       'sansursuz.info', 'sauterenfrance.nl', 'schreiben.net',\n",
       "       'sciencefun.org', 'scienceinschool.org', 'scottrealty.com',\n",
       "       'scribblelive.com', 'scuoladimaratona.it',\n",
       "       'searchengineshowdown.com', 'searchphoenixhouses.com',\n",
       "       'sedolor.es', 'sellingsouthtexas.com', 'sentinelandenterprise.com',\n",
       "       'sentrymbaconfig.blogspot.com', 'serieonline.cc',\n",
       "       'sewingfromatoz.com', 'sewwhatyvette.com', 'sexpin.net',\n",
       "       'share2fb.net', 'shirleyranch.com', 'shopotam.ru',\n",
       "       'shoppingonchampagne.com', 'shutterfly.com',\n",
       "       'simpletaxreturns.com', 'singaporeeducation.info',\n",
       "       'sisi-terang.com', 'sjdhomes.com.au', 'skyscanner.com.au',\n",
       "       'slotforum.com', 'softstore.it', 'sospsicologo.org',\n",
       "       'soundcloud.com', 'southgatv.com', 'spankjizz.com',\n",
       "       'sparkleteam.com', 'spcg.pl', 'spielesite.com', 'sprint.com',\n",
       "       'sscsr.gov.in', 'stars.coupons', 'starsessions.ml',\n",
       "       'stmaryhull.com', 'stockvault.net', 'stonecountyenterprise.com',\n",
       "       'strongvpn.com', 'studiofunghi.it',\n",
       "       'successful-horse-training-and-care.com', 'superofferte.nl',\n",
       "       'surfhungry.com', 'surreytranslation.co.uk',\n",
       "       'sweetiessecretsweeps.com', 'swesoft.com', 'swin.edu.au',\n",
       "       'swvarealty.com', 'systutorials.com', 'taghotelfano.com',\n",
       "       'talk2solicitors.co.uk', 'tanakayamato.com', 'tanium.com',\n",
       "       'tarot-study.info', 'tateproperty.com',\n",
       "       'tazkirahdaily.blogspot.com', 'tcr-group.com', 'televall.online',\n",
       "       'tellychakkar.com', 'telugunews360.com', 'tessarhodes.com',\n",
       "       'tgcom24.it', 'thaifastcash.com', 'the-smart-bet.co.uk',\n",
       "       'theanywhereoffice.com', 'theaquariumguide.com', 'thebandha.com',\n",
       "       'thebossykitchen.com', 'thecakechica.com',\n",
       "       'thecastudentsclub.blogspot.com', 'thecouponproject.com',\n",
       "       'thecuttingroomperth.co.uk', 'thegymtumut.com.au',\n",
       "       'thehockeywriters.com', 'thehornetsnesttattoo.com',\n",
       "       'theindoorgrill.com', 'theintelligentinvestor.net',\n",
       "       'themexicoreport.com', 'theokobojilife.com',\n",
       "       'thepremierhomegroup.com', 'thesimplehomeschooler.com',\n",
       "       'thetamilyogi.co', 'thewaldenword.com', 'thongaspredo.com',\n",
       "       'thumpertalk.com', 'ticaret.edu.tr', 'tikilive.com',\n",
       "       'toledoareahomes.com', 'tomlinsonbomberger.com', 'tomsarkgh.am',\n",
       "       'topky.sk', 'topxxx.xyz', 'torrentqq98.com', 'touslesconcours.ca',\n",
       "       'trailmeister.com', 'tructiepus.com', 'truenewshr.com',\n",
       "       'tusmanualidadespararegalar.com', 'tv107.com', 'tva.gov',\n",
       "       'tvseries.video', 'twinponds.com', 'uberhumor.com',\n",
       "       'ultimatehomelife.com', 'unitedwithlove.com', 'untappd.com',\n",
       "       'upgarage.com', 'upvalleyinn.com', 'usedtrucks.com.au',\n",
       "       'vacationrentalinsurance.com', 'valencia-sport.com', 'vdplaats.nl',\n",
       "       'vegancalculator.com', 'velocitycars.co.za',\n",
       "       'victoriasagency.com.ua', 'villa-guadeloupe.com',\n",
       "       'violaobrasileiro.com', 'vistacampus.gov', 'vivino.com',\n",
       "       'vnsny.org', 'voetbalbelgie.be', 'voyagesetvagabondages.com',\n",
       "       'vreelandersloep.nl', 'vrouwenvannu.nl', 'vucutgelistirmetv.com',\n",
       "       'waaytv.com', 'wabtube.net', 'walkinmygarden.com', 'watchop.io',\n",
       "       'watchrs.club', 'watsonbuckle.co.uk', 'wattar-gruppen.dk',\n",
       "       'wbldc.co.in', 'weahomes.com', 'webex.com', 'webtaxguide.net',\n",
       "       'wedivorce.fr', 'weightworld.se', 'wellnessliving.com',\n",
       "       'westelm.ae', 'whatismyipaddress.com', 'wifeupdate.com',\n",
       "       'wildernesstoday.com', 'wjbf.com', 'wjla.com', 'wkd-uk.com',\n",
       "       'world-weather.by', 'worldtvpc.com', 'worthingcourtblog.com',\n",
       "       'wpgroupurl.com', 'wsi-emarketing.com', 'xcelina.com', 'xm.com',\n",
       "       'xmecam.com', 'xmtrading.com', 'xn--22c6b6a1dr8i.com',\n",
       "       'yalewestapts.com', 'yogapoint.com', 'yokohama-cu.ac.jp',\n",
       "       'yorkpress.co.uk', 'ziyad.info', 'zoox18.com',\n",
       "       'zostalampsychologiem.pl', 'zuberrealty.com'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df['domain'].unique()))\n",
    "df['domain'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lcrf.org                     50\n",
       "latineuro.com                50\n",
       "mrbnursesmemes.in            50\n",
       "movieszaa.com                50\n",
       "motleys.com                  50\n",
       "                             ..\n",
       "thesimplehomeschooler.com     1\n",
       "efit30.com                    1\n",
       "kittywire.com                 1\n",
       "atheistrev.com                1\n",
       "mythicowl.com                 1\n",
       "Name: domain, Length: 662, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  domain                     \n",
       "IAB1   7starhd.red                    50\n",
       "IAB21  kinghomes.co.uk                50\n",
       "       sellingsouthtexas.com          50\n",
       "       searchphoenixhouses.com        50\n",
       "       scottrealty.com                50\n",
       "                                      ..\n",
       "IAB25  cute-girl-nude.xyz              1\n",
       "       drpciv.biz                      1\n",
       "IAB13  vacationrentalinsurance.com     1\n",
       "IAB23  atheistrev.com                  1\n",
       "IAB8   beginningwithbergamot.com       1\n",
       "Length: 662, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['label', 'domain']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813      https://www.alvolante.it/listino_auto/honda-civic\n",
       "10485                            https://www.lilyhair.com/\n",
       "9726     https://www.juffrouwtaart.nl/product/funcakes-...\n",
       "343      https://abc57.com/news/police-investigating-sh...\n",
       "8369     https://homedecorativefurniture.com/author/adm...\n",
       "9677            https://www.juegos.com/juego/blob-giant-3d\n",
       "15708                     http://shirleyranch.com/booking/\n",
       "3272     https://www.camping-loperhet.com/729-camping-a...\n",
       "2670     http://www.bizpromptinfo.com/เปิดที่มา-ข้าวหม้...\n",
       "2560     https://www.bioresonantie-therapie.nl/therapie...\n",
       "8932     https://www.imperiasport.net/2021/08/03/calcio...\n",
       "11311    https://www.mijnwoordenboek.nl/puzzelwoordenbo...\n",
       "9923     https://www.keshhair.com/product/coconut-cream...\n",
       "7198     https://g6hentai.com/search/Pok%C3%A9mon%20/pa...\n",
       "1440     https://www.asortie.com/arb/classic-sofa-set/6...\n",
       "14877    https://www.ryanair.com/us/en/trip/rooms/list/...\n",
       "12218    https://newlifevitaminshop.com/?cmp_id=3467176...\n",
       "8423     https://homefurnitureandpatio.com/meadow-decor...\n",
       "5283     https://www.donatelife.gov.au/join-register?gc...\n",
       "7207     https://g6hentai.com/gallery/14465/ichika,-sek...\n",
       "12391    https://www.newstrackerdaily.com/trending?utm_...\n",
       "11978    http://www.nationalarchives.gov.uk/records/med...\n",
       "13978    https://www.premierdentalco.com/zr-cem/?utm_so...\n",
       "10401    https://www.legendsoflearning.com/?fbclid=IwAR...\n",
       "17551                     http://thehornetsnesttattoo.com/\n",
       "17018    http://tazkirahdaily.blogspot.com/2017/01/tahu...\n",
       "17134    https://www.telugunews360.com/2021/07/iron-def...\n",
       "11387    https://millyrecetas.blogspot.com/2015/01/tira...\n",
       "8815     https://www.ikrush.com/kristy-oversized-shirt-...\n",
       "82       https://365onlinenews.com/സോഷ്യല്‍-മീഡിയ-വരുന്...\n",
       "Name: urls, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['urls'].sample(n = 30, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sys 4chan wheeldecide c1 she ends up pregnant and makes you raise the kid amp c2 she ends up leaving you for them amp c3 she keeps doing it behind your back amp c4 it becomes a regular thing with your consent amp c5 it becomes a regular thing against your will amp c6 the bull goes after your mother next amp c7 the bull goes after your sister next amp c8 the bull goes after your daughter next amp c9 you are a part of it and clean her after every session amp c10 you are a part of it and clean the bull after every session amp c11 you never see her again amp c12 you provide for them financially amp c13 you watch every second of it amp c14 you 27re kept around for your money amp c15 she ends up pregnant with twins and makes you raise them amp c16 she ends up pregnant with triplets and makes you raise them amp c17 she got pozzed amp c18 you have to get her used 2c unconscious body out of a dumpster amp c19 you find her after she was used as a toilet in a public restroom amp c20 she was filmed 2c and the video is going viral amp c21 the bull moves in with you amp c22 she was drugged 2c and is now addicted amp c23 all of it was livestreamed amp c24 she never wants to have sex with you anymore amp c25 her holes have become too loose for you to fuck amp c26 she forces you into an open relationship amp c27 you agree to an open relationship amp c28 she becomes their booty call amp c29 she converts your house into her brothel amp c30 she becomes a classy escort amp c31 she becomes a trashy prostitute amp c32 she gets a tattoo as a souvenir amp c33 she gets branded to remember the bull amp c34 she needs to spend a couple of days in hospital to recuperate amp c35 she becomes their pet amp c36 you watch without them knowing amp c37 you have to take care of her puking up cum for a couple of hours amp c38 she was fucked too hard and becomes infertile amp c39 she gets knocked up 2c making her tits 2c ass 2c hips and thighs all thicker amp c40 she becomes a porn star amp t how did it end up 3f amp time 5'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = df['urls'][df['urls'].str.len().idxmax()]\n",
    "url = preprocess_data_helper(url)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_RAW = '../data/raw'\n",
    "PATH_DATA_CLEAN = '../data/cleaned/'\n",
    "PATH_DATA_TRAIN = '../data/train/'\n",
    "PATH_DATA_STATIC = '../data/static_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_helper(text):\n",
    "    # translator = str.maketrans('', ' ', string.punctuation)\n",
    "    # text_without_punctuation = text.translate(translator)\n",
    "    stop = stopwords.words('english')\n",
    "    words = ['http', 'https', 'org', 'com', 'php', 'derefer', 'url', 'index', 'www']\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(re.escape(word) for word in words))\n",
    "    processed_string = re.sub(pattern, '', text)\n",
    "    processed_string = re.sub(r'[^\\w\\s]', ' ', processed_string)\n",
    "    processed_string = re.sub(r'\\s+', ' ', processed_string)\n",
    "    processed_string = re.sub(r'[^A-Za-z0-9 ]+', ' ', processed_string)\n",
    "    processed_string = ' '.join([word for word in processed_string.split() if word not in (stop)])\n",
    "    return processed_string.lower()\n",
    "\n",
    "def read(filename):\n",
    "    return pd.read_parquet(os.path.join(PATH_DATA_RAW, filename))\n",
    "\n",
    "def preprocess_data(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # df = df.explode('urls', ignore_index = True)\n",
    "    df['domain'] = df['domain'].str.lower()\n",
    "    df['domain'] = df['domain'].apply(preprocess_data_helper)\n",
    "    df['urls'] = df['urls'].str.join(' ')\n",
    "    df['urls'] = df['urls'].str.lower()\n",
    "    df['urls'] = df['urls'].apply(preprocess_data_helper)\n",
    "    df['urls'] = df['urls'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "    return df\n",
    "\n",
    "def write(df, out_filename):\n",
    "    out_filename = out_filename.split('.')[0] + '.csv'\n",
    "    df.to_csv(os.path.join(PATH_DATA_CLEAN, out_filename), index = False)\n",
    "\n",
    "def augment_col(df):\n",
    "    df['target'], map = pd.factorize(df['label'])\n",
    "    map = dict(zip(range(len(map)), map))\n",
    "    with open(os.path.join(PATH_DATA_STATIC, 'label.pkl'), 'wb') as file:\n",
    "        pickle.dump(map, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    return df\n",
    "\n",
    "def join():\n",
    "    csv_files = glob.glob(os.path.join(PATH_DATA_CLEAN ,'*.csv'))\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        temp_df = pd.read_csv(file)\n",
    "        combined_data = combined_data.append(temp_df, ignore_index = True)\n",
    "    combined_data = augment_col(combined_data)\n",
    "    combined_data.to_csv(os.path.join(PATH_DATA_TRAIN, 'train.csv'), index = False)\n",
    "    print('Files Joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'part-00000-tid-5266091121617272357-73173b1e-ce15-42f6-a92f-e17a6426d0d7-1572-1-c000.snappy.parquet'\n",
    "df = read(filename)\n",
    "df = preprocess_data(df)\n",
    "write(df, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Joined\n"
     ]
    }
   ],
   "source": [
    "raw_files = glob.glob(os.path.join(PATH_DATA_RAW ,'*.snappy.parquet'))\n",
    "for raw_file in raw_files:\n",
    "    raw_file = raw_file.split('/')[-1]\n",
    "    df = read(raw_file)\n",
    "    df = preprocess_data(df)\n",
    "    write(df, raw_file)\n",
    "join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(X, col = 'name_title'):\n",
    "    vectorizer = CountVectorizer(stop_words = 'english')\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def one_hot_encoding(X, col = 'name_title'):\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    X = X.values.reshape(-1, 1)\n",
    "    X = one_hot_encoder.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def word_2_vector(X):\n",
    "    w2v_model = gensim.models.Word2Vec(X, vector_size = 100, window = 5, min_count = 2)\n",
    "\n",
    "def glove(X):\n",
    "    return X\n",
    "\n",
    "def tfidf(X, col = 'name_title'):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000)\n",
    "    return tfidf_vectorizer.fit_transform(X[col])\n",
    "\n",
    "def countvector_tfidtransform(X, col = ''):\n",
    "    cv = CountVectorizer(stop_words = 'english')\n",
    "    tfidf = TfidfTransformer()\n",
    "    X = cv.fit_transform(X[col])\n",
    "    return tfidf.fit_transform(X)\n",
    "    # pipeline = Pipeline([\n",
    "    #     ('vect', CountVectorizer(stop_words = 'english')),\n",
    "    #     ('tfidf', TfidfTransformer()),\n",
    "    # ])\n",
    "    # return pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    lr = LogisticRegression(C = 100.0, random_state = 1, solver = 'lbfgs', multi_class = 'ovr')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_predict = lr.predict(X_test)\n",
    "    print(y_predict)\n",
    "    print(\"Logistic Regression Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def sgd_classifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    sgd = SGDClassifier(loss = 'hinge', penalty = 'l2', alpha = 1e-3, random_state = 42, max_iter = 20, tol = None)\n",
    "    sgd.fit(X_train, y_train)\n",
    "    y_predict = sgd.predict(X_test)\n",
    "    print(\"SGD Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def linear_svc(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    lsvc = LinearSVC()\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    y_predict = lsvc.predict(X_test)\n",
    "    print(\"Linear SVC Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def knn(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_predict = knn.predict(X_test)\n",
    "    print(\"K Neighbors Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def tree(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_predict = tree.predict(X_test)\n",
    "    print(\"Decision Tree Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def nn(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    nn = MLPClassifier(n_neighbors = 3)\n",
    "    nn.fit(X_train, y_train)\n",
    "    y_predict = nn.predict(X_test)\n",
    "    print(\"MLP Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def naive_bayes(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_predict = nb.predict(X_test)\n",
    "    print(\"Naive Bayes Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def random_forest(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    random_forest = RandomForestClassifier()\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    y_predict = random_forest.predict(X_test)\n",
    "    print(\"Random Forest Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22365, 2) (22365, 1)\n",
      "True\n",
      "(16773, 2) (16773, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape, y_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pickle.dump(clf, open('model.pkl', 'wb'))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# y_predict = clf.predict(X_test)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print('Classifier Accuracy %.3f' %metrics.accuracy_score(y_test, y_predict))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# sns.heatmap(cf_matrix, annot = True, fmt='.2%', cmap = 'Blues')\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/pipeline.py:378\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 378\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    334\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 870\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "df = shuffle(pd.read_csv(os.path.join(PATH_DATA_TRAIN, 'train.csv')).dropna())\n",
    "features = ['domain', 'urls']\n",
    "X = df[features].values\n",
    "y = df[['target']].values\n",
    "# y = y.values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "clf = Pipeline([\n",
    "     ('vect', CountVectorizer(stop_words = 'english')),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', RandomForestClassifier()\n",
    ")])\n",
    "print(X.shape, y.shape)\n",
    "print(X.shape[0] == y.shape[0])\n",
    "print(X_train.shape, y_train.shape)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# pickle.dump(clf, open('model.pkl', 'wb'))\n",
    "# y_predict = clf.predict(X_test)\n",
    "# print('Classifier Accuracy %.3f' %metrics.accuracy_score(y_test, y_predict))\n",
    "# print('Balanced Classifier Accuracy %.3f' %metrics.balanced_accuracy_score(y_test, y_predict))\n",
    "# print('f1_score %.3f' %metrics.f1_score(y_test, y_predict, average = 'micro'))\n",
    "# print('Precision %.3f' %metrics.precision_score(y_test, y_predict, average = 'micro'))\n",
    "# print('Recall %.3f' %metrics.recall_score(y_test, y_predict, average = 'micro'))\n",
    "# cf_matrix = metrics.confusion_matrix(y_test, y_predict)\n",
    "\n",
    "# sns.heatmap(cf_matrix, annot = True, fmt='.2%', cmap = 'Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
